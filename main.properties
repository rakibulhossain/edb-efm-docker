# Copyright EnterpriseDB Corporation, 2013-2025. All Rights Reserved.

# Do not use quotes around property values in this file.
# All user scripts specified below are run as the Failover Manager user.

# The value for the password property should be the output from
# 'efm encrypt' -- do not include a cleartext password here. To
# prevent accidental sharing of passwords among clusters, the cluster
# name is incorporated into the encrypted password. If you change
# the cluster name (the name of this file), you must encrypt the
# password again with the new name.
# The db.port property must be the same for all nodes.
db.user=efm
db.password.encrypted=bddeb7f07da8faeba5dbc4f233655441
db.port=5432
db.database=postgres

# This property tells EFM which OS user owns the $PGDATA dir for the
# 'db.database'.  By default, the owner is either 'postgres' for PostgreSQL
# or 'enterprisedb' for EDB Postgres Advanced Server.  However, if you
# have configured your db to run as a different user, you will need to
# copy the /etc/sudoers.d/efm-XX conf file to grant the
# necessary permissions to your db owner.
#
# This username must have write permission to the 'db.data.dir'
# specified below.
db.service.owner=postgres

# Specify the proper service name in order to use service commands rather
# than pg_ctl to start/stop/restart a database. For example, if this property
# is set, then 'service <name> restart' or 'systemctl restart <name>'
# (depending on OS version) will be used to restart the database rather than pg_ctl.
# This property is required if running the database as a service.
db.service.name=

# Specify the directory containing the pg_controldata/pg_ctl commands, for example:
# /usr/edb/as15/bin. Unless the db.service.name property is used, the pg_ctl
# command is used to start/stop/restart databases as needed after a
# failover or switchover. This property is required.
db.bin=/usr/lib/postgresql/16/bin

# This is the directory where a standby.signal file will exist for a standby node.
# If a primary database fails, a recovery.conf file will be written in this
# location to ensure that the failed database can not be restarted as the
# primary database.
# This corresponds to database environment variable PGDATA and should be same
# as the output of query 'show data_directory;' on respective database.
db.data.dir=/var/lib/postgresql/data

# Specify the location of database configuration files if they are
# not contained in the same location as the standby.signal
# file. This is most likely the case for Debian installations. The location
# specified will be used as the -D value (the location of the data directory
# for the cluster) when calling pg_ctl to start or stop the database.
# If this property is blank, the db.data.dir location specified by the
# db.data.dir property will be used.
# This corresponds to the output of query 'show config_file;' on respective database.
db.config.dir=

# Use the jdbc.sslmode property to enable ssl for EFM connections. Setting
# this property to anything but 'disable' will force the agents to use
# 'ssl=true' for all JDBC database connections (to both local and remote
# databases). Valid values are:
#
# disable   - Do not use ssl for connections.
# verify-ca - EFM will perform CA verification before allowing the
#             certificate.
# require   - Verification will not be performed on the server certificate.
jdbc.sslmode=disable

# Use the jdbc.properties property to pass extra name/value pairs
# to JDBC when creating a database connection. An example of this
# format is:
# jdbc.properties=prop1=value1 prop2=value2 prop3=value3
jdbc.properties=

# Email address(es) for notifications. The value of this property must
# be the same across all agents. Multiple email addresses must
# be separated by space. This is required if not using a 'script.notification'
# script. Either/both can be used.
user.email=unknown@gmail.com

# Use the from.email property to specify the from email address that
# will be used for email notifications. Use the %h placeholder to
# represent the name of the node host (e.g. example@%h). The
# placeholder will be replaced with the name of the host as returned
# by the hostname command.
# Leave blank to use the default, efm@localhost.
from.email=

# Minimum severity level of notifications that will be sent by the agent.
# The minimum level also applies to the notification script (below).
# Valid values are INFO, WARNING, and SEVERE. A list of notifications is
# grouped by severity in the user's guide.
notification.level=INFO

# Text to add to the beginning of every notification. This could be used to help identify
# what the cluster is used for, the role of this node, etc. To use multiple lines, add
# a backslash \ to the end of a line of text. To include a newline use \n.
# Example:
# notification.text.prefix=Development cluster for Example department.\n\
#  Used by Dev and QA \
#  See Example group for questions.
notification.text.prefix=

# Absolute path to script run for user notifications.
#
# This is an optional user-supplied script that can be used for
# notifications instead of email. This is required if not using
# email notifications. Either/both can be used. The script will
# be passed two parameters: the message subject and the message body.
script.notification=

# This property specifies the ip address and port that jgroups
# will bind to on this node. The value is of the form <ip>:<port>.
# Note that the port specified here is used for communicating with
# other nodes, and is not the same as the admin.port below, used
# only to communicate with the local agent to send control signals.
# For example, <provide_your_ip_address_here>:7800
bind.address=172.16.14.122:7800

# This is the ip address/hostname to be used for communication with all
# other Failover Manager agents. All traffic towards this address should
# be routed by the network to the bind.address of the node.
# The value is in the ip/hostname format only. This address will be used
# in scenarios where nodes are on different networks and broadcast an IP
# address other than the bind.address to the external world.
external.address=

# This property controls the port binding of the administration server
# which is used for some commands (ie cluster-status). The default
# is 7809; you can modify this value if the port is already in use.
admin.port=7809

# Specifies whether or not this is a witness node. Witness nodes
# do not have local databases running.
is.witness=False

# These properties apply to the connection(s) EFM uses to monitor
# the local database. Every 'local.period' seconds, a database check
# is made in a background thread. If the main monitoring thread does
# not see that any checks were successful in 'local.timeout' seconds,
# then the main thread makes a final check with a timeout value
# specified by the 'local.timeout.final' value. All values are in seconds.
# Whether EFM uses single or multiple connections for database checks
# is controlled by the 'db.reuse.connection.count' property.
local.period=3
local.timeout=5
local.timeout.final=5

# Timeout for a call to check if a remote database is responsive.
# For example, this is how long a standby would wait for a
# DB ping request from itself and the witness to the primary DB
# before performing failover.
remote.timeout=5

# The total amount of time in seconds to wait before determining that
# a node has failed or been disconnected from this node.
#
# The value of this property must be the same across all agents.
node.timeout=5

# Set to true to encrypt messages that are sent between agents.
# This property must be the same on all agents or else the agents
# will not be able to connect.
encrypt.agent.messages=false

# Whether or not the 'efm stop-cluster <cluster name>' command is enabled.
# Set to false to disable the command, in which case all Failover
# Manager agents must be stopped individually. Note that stopping each
# agent separately will change the .nodes files on remaining agents
# unless stable.nodes.file is also true. This property value must
# be the same on all agents if set. The default is true if not set.
enable.stop.cluster=true

# Shut down the database after a primary agent detects that it has
# been isolated from the majority of the efm cluster. If set to
# true, efm will stop the database before running the
# 'script.primary.isolated' script, if a script is specified.
stop.isolated.primary=true

# Attempt to shut down a failed primary database after EFM can no longer
# connect to it. This can be used for added safety in the case a
# failover is caused by a failure of the network on the primary node.
# If specified, a 'script.db.failure' script is run after this attempt.
stop.failed.primary=true

# Treat a primary agent shutdown as an agent failure. This can be set to true
# to treat a primary agent shutdown as a failure situation. Caution should be
# used when using this feature, as it could cause an unwanted
# promotion in the case of performing primary database maintenance.
# Please see the user's guide for more information.
primary.shutdown.as.failure=True

# Period in seconds between having the primary agent update promotable
# standbys with physical replication slot information so that
# the cluster will continue to use replication slots after a failover.
# Set to zero to turn off.
update.physical.slots.period=5

# This is the address of a well-known server that EFM can ping in an
# effort to determine network reachability issues.  It might be the IP
# address of a nameserver within your corporate firewall or another server
# that *should* always be reachable via a 'ping' command from each of the
# EFM nodes.
#
# There are many reasons why this node might not be considered reachable:
# firewalls might be blocking the request, ICMP might be filtered out,
# etc.
#
# Do not use the IP address of any node in the EFM cluster (primary, standby,
# or witness) because this ping server is meant to provide an additional
# layer of information should the EFM nodes lose sight of each other.
#
# The installation default is Google's DNS server.
ping.server.ip=8.8.8.8

# This command will be used to test the reachability of certain nodes.
#
# Do not include an IP address or hostname on the end of this command - it
# will be added dynamically at runtime with the values contained in 'virtual.ip'
# and 'ping.server.ip'.
#
# Make sure this command returns reasonably quickly - test it from a shell
# command line first to make sure it works properly.
ping.server.command=/bin/ping -q -c3 -w5

# Have the first node started automatically add the addresses from
# its .nodes file to the allowed host list. This will make it
# faster to start the cluster when the initial set of hosts
# is already known.
auto.allow.hosts=true

# When set to true, EFM will not rewrite the .nodes file whenever new nodes
# join or leave the cluster. This can help starting a cluster in the cases
# where it is expected for member addresses to be mostly static, and combined
# with 'auto.allow.hosts' makes startup easier when learning Failover Manager.
stable.nodes.file=true

# This property controls how many times a database connection is reused
# before creating a new one. If set to zero, a new connection will be
# created every time an agent pings its local database.
db.reuse.connection.count=0

# Whether or not failover will happen automatically when the primary
# fails. Set to false if you want to receive the failover notifications
# but not have EFM actually perform the failover steps.
# The value of this property must be the same across all agents.
auto.failover=true

# After a standby is promoted, Failover Manager will attempt to
# update the remaining standbys to use the new primary. Failover
# Manager will change the host parameter of the primary_conninfo
# entry in postgresql.auto.conf and restart the database. The
# restart command is contained in either the efm_db_functions or
# efm_root_functions file; default when not running db as an os
# service is: "pg_ctl restart -m fast -w -t <timeout> -D <directory>"
# where the timeout is the local.timeout property value and the
# directory is specified by db.data.dir. To turn off
# automatic reconfiguration, set this property to false.
auto.reconfigure=true

# A standby with this set to false will not be added to the failover
# priority list, and so will not be available for promotion. The
# property will be used whenever an agent starts as a standby or
# resumes as a standby after being idle. After startup/resume, the
# node can still be added or removed from the priority list with the
# 'efm set-priority' command. This property is required for all
# non-witness nodes.
promotable=true

# Use replay LSN value for tiebreaker when choosing a standby to promote
# before using failover priority. Set this property to true to consider
# replay location as more important than failover priority (as seen in
# cluster-status command) when choosing the "most ahead" standby to promote.
use.replay.tiebreaker=true

# Time in seconds for this standby to delay restarting to follow the
# primary after a promotion. This can be used to have standbys restart
# at different times to increase availability. Caution should be used
# when using this feature, as a delayed standby will not be following
# the new primary and care must be taken that the new primary retains
# enough WAL for the standby to follow it.
# Please see the user's guide for more information.
standby.restart.delay=0

# During a switchover, recovery settings are copied from a standby
# to the original primary. If the application.name property is set,
# Failover Manager will replace the application_name portion of the
# primary_conninfo entry with this property value before starting
# the original primary database as a standby. If this property is
# not set, Failover Manager will remove the parameter value
# from primary_conninfo.
application.name=edb_test_2

# If the restore_command on a standby restores directly from the primary node, use this property
# to have Failover Manager change the command when a new primary is promoted.
#
# Use the %h placeholder to represent the address of the new primary. During promotion
# it will be replaced with the address of the new primary.
#
# If not specified, failover manager will not change the restore_command value, if any,
# on standby nodes.
#
# Example:
# restore.command=scp <db service owner>@%h:/var/lib/edb/as15/data/archive/%f %p
restore.command=

# When a standby is reconfigured to follow a new primary, its local WAL
# is removed. Set this property to true to create a backup of the WAL first.
# This is generally not necessary; the property has been added to allow
# backwards compatibility with the behavior of versions of Failover Manager
# before version 5. If not specified defaults to false.
backup.wal=false

# Reduce num_sync when the number of synchronous standbys drops below the
# value required by the primary database. If set to true, Failover Manager will
# reduce the number of standbys needed in the primary's
# synchronous_standby_names property and reload the primary configuration.
# Failover Manager will not reduce the number below 1, taking the primary
# out of synchronous replication, unless the reconfigure.sync.primary
# property is also set to true.
# To raise num_sync, see the reconfigure.num.sync.max property below.
reconfigure.num.sync=false

# If reconfigure.num.sync is set to true and this property is set, Failover
# Manager will check if num_sync can be raised when a standby is added to the cluster.
# Failover Manager will not raise the value above the maximum set here.
# If the primary database has been taken out of synchronous mode completely (see
# the reconfigure.sync.primary property), then Failover Manager will not
# reconfigure the primary database if standbys are added to the cluster.
reconfigure.num.sync.max=

# Take the primary database out of synchronous replication mode when needed.
# If set to true, Failover Manager will clear the synchronous_standby_names
# configuration parameter on the primary if the number of synchronous
# standbys drops below the required level for the primary to accept writes.
# If set to false, Failover Manager will detect the situation but will only
# send a notification if the standby count drops below the required level.
#
# CAUTION: TAKING THE PRIMARY DATABASE OUT OF SYNCHRONOUS MODE MEANS THERE
# MAY ONLY BE ONE COPY OF DATA. DO NOT MAKE THIS CHANGE UNLESS YOU ARE SURE
# THIS IS OK.
reconfigure.sync.primary=false

# Period in seconds between checking if the primary database has more or fewer
# synchronous standbys than it requires. If the reconfigure.num.sync or
# reconfigure.sync.primary properties are used, Failover Manager will change
# synchronous_standby_names on the primary as needed. If the primary
# database is not in synchronous mode, this property has no effect.
check.num.sync.period=30

# Instead of setting specific standbys as being unavailable for
# promotion, this property can be used to set a minimum number
# of standbys that will not be promoted. Set to one, for example,
# promotion will not happen if it will drop the number of standbys
# below this value. This property must be the same on each node.
minimum.standbys=0

# Space-separated list of standby addresses that are high priority for
# promotion when this node is the primary. If set, when this node is
# promoted, addresses in this list will be added to the front of the
# standby priority list. If this list contains addresses that are not
# standbys at the time of promotion, they will not be added.
priority.standbys=

# Time in seconds between checks to see if a promoting database
# is out of recovery.
recovery.check.period=1

# Time in seconds to keep trying to connect to a database after a start
# or restart command returns successfully but the database is not
# ready to accept connections yet (a rare occurrence). This applies to
# standby databases that are restarted when being reconfigured for a new
# primary, and to primary databases that are stopped and started as
# standbys during a switchover.
#
# This retry mechanism is unrelated to the auto.resume.*.period parameters.
restart.connection.timeout=60

# Period in seconds for agents to try to resume monitoring when starting
# in IDLE mode. Set to 0 for agents to not try to resume (in which case
# the 'efm resume <cluster>' command is used after bringing a database up).
auto.resume.startup.period=0

# Period in seconds for IDLE agents to try to resume monitoring after
# a database failure. Set to 0 for agents to not try to resume (in
# which case the 'efm resume <cluster>' command is used after
# bringing a database back up).
auto.resume.failure.period=0

# These properties specify the IP and prefix length that will be remapped during
# failover. If you do not use a VIP as part of your failover solution, leave
# the virtual.ip property blank to disable Failover Manager support for VIP processing
# (assigning, releasing, testing reachability, etc).
#
# If you specify a VIP, the interface and prefix are required.
#
# If you specify a host name, it will be resolved to an IP address when acquiring or
# releasing the VIP. If the host name resolves to more than one IP address,
# there is no way to predict which address Failover Manager will use.
#
# By default, the virtual.ip and virtual.ip.prefix values must be the same
# across all agents. If you set virtual.ip.single to false, you can specify unique values
# for virtual.ip and virtual.ip.prefix on each node.
#
# If you are using an IPv4 address, the virtual.ip.interface value
# should not contain a secondary virtual ip id (do not include ":1", etc).
virtual.ip=172.16.14.220
virtual.ip.interface=ens192
virtual.ip.prefix=24
virtual.ip.single=true

# Whether to check if the VIP (when used) is still in use before promoting after
# a primary failure. Turning this off may allow the new primary to have the VIP
# even though another node is also broadcasting it. This should only be used
# in environments where it is known that the failed primary node will be isolated
# or shut down through other means.
check.vip.before.promotion=true


# A boolean property to enable Failover Manager managed pgpool HA.
#
# If enabled, Failover Manager would natively update the joining and leaving status
# of database nodes to active pgpool instance.
# Failover manager expects properly configured and running pgpool instances on
# required nodes. It does not manage setup and configuration of pgpool on any node.
#
# By default the property is disabled.
pgpool.enable=false

# Configurations required for pgpool integration.
#
# 'pcp.user' - User that would be invoking PCP commands
# 'pcp.host' - Virtual IP that would be used by pgpool. Same as pgpool parameter 'delegate_IP'
# 'pcp.port' - The port on which pgpool listens for pcp commands.
# 'pcp.pass.file' - Absolute path of PCPPASSFILE.
# 'pgpool.bin' - Absolute path of pgpool bin directory
#
# These properties are required if 'pgpool.enable' is set to true.
pcp.user=
pcp.host=
pcp.port=
pcp.pass.file=
pgpool.bin=


# Absolute path to load balancer scripts
#
# The attach script is called when a node should be attached to the load
# balancer, for example after a promotion. The detach script is called
# when a node should be removed, for example when a database has failed
# or is about to be stopped. Use %h to represent the IP/hostname
# of the node that is being attached/detached. Use %t to represent the
# type of node being attached or detached: the letter p will be passed
# in for primary nodes and the letter s for standby nodes.
#
# Example:
# script.load.balancer.attach=/somepath/attachscript %h %t
script.load.balancer.attach=
script.load.balancer.detach=

# If set to true, Failover Manager will detach the node from load balancer if the primary
# agent fails but the database is still reachable. In most scenarios this is NOT the desired situation.
# In scenarios where the detach script should run with a failed primary agent, even when the primary
# database is still healthy this parameter should be set to true.
#
# If no value specified it defaults to false.
detach.on.agent.failure=

# Absolute path to fencing script run during promotion
#
# This is an optional user-supplied script that will be run during
# failover on the standby database node.  If left blank, no action will
# be taken.  If specified, EFM will execute this script before promoting
# the standby.
#
# Parameters can be passed into this script for the failed primary and
# new primary node addresses. Use %p for new primary and %f for failed
# primary. On a node that has just been promoted, %p should be the same
# as the node's efm binding address.
#
# Example:
# script.fence=/somepath/myscript %p %f
#
# NOTE: FAILOVER WILL NOT OCCUR IF THIS SCRIPT RETURNS A NON-ZERO EXIT CODE.
script.fence=

# Absolute path to fencing script run after promotion
#
# This is an optional user-supplied script that will be run after
# failover on the standby node after it has been promoted and
# is no longer in recovery. The exit code from this script has
# no effect on Failover Manager, but will be included in a notification
# sent after the script executes.
#
# Parameters can be passed into this script for the failed primary and
# new primary node addresses. Use %p for new primary and %f for failed
# primary. On a node that has just been promoted, %p should be the same
# as the node's efm binding address.
#
# Example:
# script.post.promotion=/somepath/myscript %f %p
script.post.promotion=

# Absolute path to resume script
#
# This script is run before an IDLE agent resumes
# monitoring its local database.
script.resumed=

# Absolute path to script run after database failure
#
# This is an optional user-supplied script that will be run after
# an agent detects that its local database has failed.
script.db.failure=

# Absolute path to script run on isolated primary
#
# This is an optional user-supplied script that will be run after
# a primary agent detects that it has been isolated from the majority
# of the efm cluster.
script.primary.isolated=

# Absolute path to script invoked on non-promoting agent nodes
# before a promotion.
#
# This optional user-supplied script will be invoked on other agents
# when a node is about to promote its database. The exit code from this script has
# no effect on Failover Manager, but will be included in a notification
# sent after the script executes.
#
# Pass a parameter (%p) with the script to identify the new primary node address.
#
# Example:
# script.remote.pre.promotion=/path_name/script_name %p
script.remote.pre.promotion=

# Absolute path to script invoked on non-primary agent nodes
# after a promotion.
#
# This optional user-supplied script will be invoked on nodes
# (except the new primary) after a promotion occurs. The exit code from this script has
# no effect on Failover Manager, but will be included in a notification
# sent after the script executes.
#
# Pass a parameter (%p) with the script to identify the new primary node address.
#
# Example:
# script.remote.post.promotion=/path_name/script_name %p
script.remote.post.promotion=

# Absolute path to a custom monitoring script.
#
# Use script.custom.monitor to specify the location and name of an optional
# user-supplied script that will be invoked periodically to perform custom
# monitoring tasks. A non-zero exit value means that a check has failed;
# this will be treated as a database failure.  On a primary node, script
# failure will cause a promotion.  On a standby node script failure will
# generate a notification and the agent will become IDLE.
#
# The custom.monitor.* properties are required if a custom monitoring script
# is specified:
#
# custom.monitor.interval is the time in seconds between executions of the script.
#
# custom.monitor.timeout is a timeout value in seconds for how long the script
# will be allowed to run.  If script execution exceeds the specified time, the
# task will be stopped and a notification sent. Subsequent runs will continue.
#
# If custom.monitor.safe.mode is set to true, non-zero exit codes from the
# script will be reported but will not cause a promotion or be treated as a
# database failure. This allows testing of the script without affecting EFM.
#
script.custom.monitor=
custom.monitor.interval=
custom.monitor.timeout=
custom.monitor.safe.mode=

# Command to use in place of 'sudo' if desired when efm runs
# the efm_db_functions, efm_root_functions, or efm_address scripts.
# Sudo is used in the following ways by efm:
#
# sudo /usr/edb/efm-<version>/bin/efm_address <arguments>
# sudo /usr/edb/efm-<version>/bin/efm_root_functions <arguments>
# sudo -u <db service owner> /usr/edb/efm-<version>/bin/efm_db_functions <arguments>
#
# 'sudo' in the first two examples will be replaced by the value of the sudo.command
# property. 'sudo -u <db service owner>' will be replaced by the value of the
# sudo.user.command property. The '%u' field will be replaced with the db owner.
sudo.command=sudo
sudo.user.command=sudo -u %u

# Specify the directory of lock file on the node. Failover Manager creates a
# file named <cluster>.lock at this location to avoid starting multiple agents
# for same cluster. If the path does not exist, Failover Manager will attempt
# to create it. If not specified defaults to '/var/lock/efm-<version>'
lock.dir=

# Specify the directory to create the pid file on the node. If this is changed
# from the default location, the unit file (if used) should be changed for
# the service to match. If not specified defaults to '/var/run/efm-<version>'
pid.dir=

# Specify the directory of agent logs on the node. If the path does not exist,
# Failover Manager will attempt to create it. If not specified defaults to
# '/var/log/efm-<version>'. (To store Failover Manager
# startup logs in a custom location, modify the path in the service script
# to point to an existing, writable directory.)
# If using a custom log directory, you must configure logrotate separately.
# Use 'man logrotate' for more information.
log.dir=

# Syslog information. The syslog service must be listening on the port
# for the given protocol, which can be UDP or TCP. The facilities supported
# are LOCAL0 through LOCAL7.
syslog.host=localhost
syslog.port=514
syslog.protocol=UDP
syslog.facility=LOCAL1

# Which logging is enabled.
file.log.enabled=true
syslog.enabled=false

# Logging levels for JGroups and EFM.
# Valid values are: TRACE, DEBUG, INFO, WARN, ERROR
# Default value: INFO
# It is not necessary to increase these values unless debugging a specific
# issue. If nodes are not discovering each other at startup, increasing
# the jgroups level to DEBUG will show information about the TCP connection
# attempts that may help diagnose the connection failures.
# TRACE level logging should be used for diagnosing problems only.
# It is not supported for production use.
jgroups.loglevel=INFO
efm.loglevel=INFO

# Extra information that will be passed to the JVM when starting the agent.
jvm.options=-Xmx128m
